{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # 2021年9月23日 该模型用于问题300的数据生成\n",
    "该WCGAN网络模型处理的数据是一周的用电数据，\n",
    "生成一周的用电数据 即D网络的输入数据是7*24+2位标签，输出是1或0，判断该模型是否为真实数据 G网络的输入是（1 100 ）的随机噪声和2位的随机标签，输出是724的用电数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor, Lambda, Compose\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import torch.utils.data as Data\n",
    "from torchvision.transforms import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.set_device(0)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "LEARNING_RATE = 1e-4\n",
    "BATCH_SIZE=32\n",
    "IMG_SIZE_L=7\n",
    "IMG_SIZE_H=48\n",
    "CHANNELS_IMG=1\n",
    "NUM_CLASSES = 4\n",
    "GEN_EMBEDDING = 100\n",
    "Z_DIM=100\n",
    "NUM_EPOCHS=30\n",
    "FEATURES_DISC=16\n",
    "FEATURES_GEN=16\n",
    "CRITIC_ITERATIONS =5\n",
    "LAMBDA_GP = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataSet(Dataset):\n",
    "    def __init__(self,csv_file):\n",
    "        self.data_df=pd.read_csv(csv_file,header=0)\n",
    "        #self.transform = transofrms\n",
    "        pass\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data_df)\n",
    "    \n",
    "    def __getitem__(self,index):\n",
    "        label = int(self.data_df.iloc[index,1])\n",
    "        target = torch.zeros((4))\n",
    "        target[label]=1.0\n",
    "        \n",
    "        image_values = torch.FloatTensor(self.data_df.iloc[index,10:].values)/12.469\n",
    "        image_values = torch.FloatTensor(image_values).view(1,7,48)\n",
    "        sample = {'label':label,'image_values':image_values,'target':target}\n",
    "        #return sample\n",
    "        return label,image_values,target,sample\n",
    "        \n",
    "    \n",
    "    def plot_image(self,index):\n",
    "        arr = torch.FloatTensor(self.data_df.iloc[index,10:].values).reshape(7,48)/12.469\n",
    "        plt.title(\"label=\" + str(self.data_df.iloc[index,1]))\n",
    "        \n",
    "        plt.imshow(arr,interpolation = 'none',cmap = 'Blues')\n",
    "        #plt.savefig('WeeklyData7*48.pdf',bbox_inches='tight')\n",
    "        pass\n",
    "    \n",
    "#     def plot_figure(self,index):\n",
    "#         y1 = torch.FloatTensor(self.data_df.iloc[index,10:].values)\n",
    "#         x1 = range(1,337)\n",
    "#         plt.title(\"label=\" + str(self.data_df.iloc[index,2]))\n",
    "#         plt.plot(x1,y1)\n",
    "#         #plt.savefig('WeeklyData336.pdf',bbox_inches='tight')\n",
    "#          plt.show()\n",
    "        \n",
    "#         pass\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch_dataset= MyDataSet('./Consumption_data_to_train_WDCGAN*.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader =Data.DataLoader(torch_dataset,batch_size=BATCH_SIZE,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_penalty(critic,labels,real,fake,device=\"cpu\"):\n",
    "    BATCH_SIZE,C,H,W = real.shape\n",
    "    epsilon = torch.rand((BATCH_SIZE,1,1,1)).repeat(1,C,H,W).to(device)\n",
    "    interpolated_images = real*epsilon + fake*(1-epsilon)\n",
    "    \n",
    "    mixed_scores = critic(interpolated_images,labels)\n",
    "    \n",
    "    gradient = torch.autograd.grad(\n",
    "        inputs = interpolated_images,\n",
    "        outputs = mixed_scores,\n",
    "        grad_outputs = torch.ones_like(mixed_scores),\n",
    "        create_graph = True,\n",
    "        retain_graph = True,      \n",
    "    )[0]\n",
    "    \n",
    "    \n",
    "    gradient = gradient.view(gradient.shape[0],-1)\n",
    "    gradient_norm = gradient.norm(2,dim =1)\n",
    "    gradient_penalty = torch.mean((gradient_norm -1)**2)\n",
    "    return gradient_penalty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self,channels_img,features_d,num_classes,img_size_L,img_size_H):\n",
    "        super(Discriminator,self).__init__()\n",
    "        self.img_size_L = img_size_L\n",
    "        self.img_size_H = img_size_H\n",
    "        self.disc = nn.Sequential(\n",
    "                              # input N*channels_img*7*48\n",
    "            nn.Conv2d(channels_img+1,features_d,kernel_size=2,stride=2,padding=1), #4*25\n",
    "            nn.LeakyReLU(0.2),\n",
    "            \n",
    "            self._block(features_d,features_d*2,2,2,0),#2*12\n",
    "            self._block(features_d*2,features_d*4,2,2,0),#1*6\n",
    "            nn.Conv2d(features_d*4,1,kernel_size=(1,6),stride=1,padding=0), #1*1       \n",
    "                \n",
    "        )\n",
    "        self.embed = nn.Embedding(num_classes,img_size_L*img_size_H)\n",
    "    def _block(self,in_channels,out_channels,kernel_size,stride,padding):\n",
    "        return nn.Sequential(\n",
    "        nn.Conv2d(\n",
    "        in_channels,\n",
    "        out_channels,\n",
    "        kernel_size,\n",
    "        stride,\n",
    "        padding,\n",
    "        bias=False,),\n",
    "        nn.InstanceNorm2d(out_channels,affine=True),\n",
    "        nn.LeakyReLU(0.2),\n",
    "        )\n",
    "    def forward(self,x,labels):\n",
    "        \n",
    "        embedding = self.embed(labels).view(labels.shape[0],1,self.img_size_L,self.img_size_H)\n",
    "        x = torch.cat([x,embedding],dim=1)\n",
    "        return self.disc(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self,\n",
    "                 channels_noise,\n",
    "                 channels_img,\n",
    "                 features_g,\n",
    "                 num_classes,\n",
    "                 img_size_L,\n",
    "                 img_size_H,                 \n",
    "                 embed_size,\n",
    "                ):\n",
    "        super(Generator,self).__init__()\n",
    "        self.img_size_L = img_size_L\n",
    "        self.img_size_H = img_size_H\n",
    "        self.gen = nn.Sequential(\n",
    "                            # input N*200*1*1\n",
    "            self._block(channels_noise+embed_size,features_g*16,(1,6),1,0),#N*256*1*6\n",
    "            self._block(features_g*16,features_g*8,2,2,0),#N*128*2*12\n",
    "            self._block(features_g*8,features_g*4,2,2,0),#N*64*4*24\n",
    "            \n",
    "            \n",
    "            nn.ConvTranspose2d(features_g*4,channels_img,kernel_size=(3,2),stride=2,padding=(1,0)) ,#N*1*7*48     \n",
    "            nn.Tanh(),#[-1,1]\n",
    "        )\n",
    "        self.embed = nn.Embedding(num_classes,embed_size)\n",
    "        \n",
    "    def _block(self,in_channels,out_channels,kernel_size,stride,padding):\n",
    "        return nn.Sequential(\n",
    "        nn.ConvTranspose2d(\n",
    "        in_channels,\n",
    "        out_channels,\n",
    "        kernel_size,\n",
    "        stride,\n",
    "        padding,\n",
    "        bias=False),\n",
    "        nn.BatchNorm2d(out_channels),\n",
    "        nn.ReLU(),\n",
    "        )\n",
    "    def forward(self,x,labels):\n",
    "        #latent vector z:N*noise_dim*1*1\n",
    "        embedding = self.embed(labels).unsqueeze(2).unsqueeze(3)\n",
    "        x = torch.cat([x,embedding],dim =1)\n",
    "        return self.gen(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_weights(model):\n",
    "    for m in model.modules():\n",
    "        if isinstance(m,(nn.Conv2d,nn.ConvTranspose2d,nn.BatchNorm2d)):\n",
    "            nn.init.normal_(m.weight.data,0.0,0.02)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen = Generator(Z_DIM,CHANNELS_IMG,FEATURES_GEN,NUM_CLASSES,IMG_SIZE_L,IMG_SIZE_H,GEN_EMBEDDING).to(device)\n",
    "critic = Discriminator(CHANNELS_IMG,FEATURES_GEN,NUM_CLASSES,IMG_SIZE_L,IMG_SIZE_H).to(device)\n",
    "initialize_weights(gen)\n",
    "initialize_weights(critic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_gen = optim.Adam(gen.parameters(),lr=LEARNING_RATE,betas=(0.0,0.999))\n",
    "opt_critic = optim.Adam(critic.parameters(),lr=LEARNING_RATE,betas=(0.0,0.999))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "fixed_noise = torch.randn(32,Z_DIM,1,1).to(device)\n",
    "writer_real = SummaryWriter(f\"WGANs/displcy_the_real\")\n",
    "writer_fake = SummaryWriter(f\"WGANs/displcy_the_fake\")\n",
    "step = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0/30] Batch 0/1250                Loss D: 0.3775,loss G: 0.0041\n",
      "Epoch [0/30] Batch 100/1250                Loss D: -0.1152,loss G: 0.9593\n",
      "Epoch [0/30] Batch 200/1250                Loss D: -0.1199,loss G: 0.6394\n",
      "Epoch [0/30] Batch 300/1250                Loss D: -0.0656,loss G: 0.6196\n",
      "Epoch [0/30] Batch 400/1250                Loss D: -0.0773,loss G: 0.0513\n",
      "Epoch [0/30] Batch 500/1250                Loss D: -0.0805,loss G: -0.0647\n",
      "Epoch [0/30] Batch 600/1250                Loss D: -0.0480,loss G: -0.2418\n",
      "Epoch [0/30] Batch 700/1250                Loss D: -0.0752,loss G: -0.6688\n",
      "Epoch [0/30] Batch 800/1250                Loss D: -0.0635,loss G: -0.8514\n",
      "Epoch [0/30] Batch 900/1250                Loss D: -0.0697,loss G: -0.9381\n",
      "Epoch [0/30] Batch 1000/1250                Loss D: -0.0577,loss G: -0.6677\n",
      "Epoch [0/30] Batch 1100/1250                Loss D: -0.0573,loss G: -0.9241\n",
      "Epoch [0/30] Batch 1200/1250                Loss D: -0.0179,loss G: -0.8481\n",
      "Epoch [1/30] Batch 0/1250                Loss D: -0.0249,loss G: -1.0529\n",
      "Epoch [1/30] Batch 100/1250                Loss D: -0.0415,loss G: -0.9136\n",
      "Epoch [1/30] Batch 200/1250                Loss D: -0.0097,loss G: -0.9277\n",
      "Epoch [1/30] Batch 300/1250                Loss D: -0.0149,loss G: -1.0164\n",
      "Epoch [1/30] Batch 400/1250                Loss D: 0.0332,loss G: -0.9288\n",
      "Epoch [1/30] Batch 500/1250                Loss D: -0.0553,loss G: -1.1291\n",
      "Epoch [1/30] Batch 600/1250                Loss D: -0.0206,loss G: -1.4007\n",
      "Epoch [1/30] Batch 700/1250                Loss D: -0.0460,loss G: -1.2514\n",
      "Epoch [1/30] Batch 800/1250                Loss D: -0.0281,loss G: -0.8729\n",
      "Epoch [1/30] Batch 900/1250                Loss D: -0.0371,loss G: -1.1730\n",
      "Epoch [1/30] Batch 1000/1250                Loss D: -0.0144,loss G: -1.7824\n",
      "Epoch [1/30] Batch 1100/1250                Loss D: -0.0082,loss G: -2.0466\n",
      "Epoch [1/30] Batch 1200/1250                Loss D: -0.0581,loss G: -1.1912\n",
      "Epoch [2/30] Batch 0/1250                Loss D: -0.0264,loss G: -1.5976\n",
      "Epoch [2/30] Batch 100/1250                Loss D: -0.0075,loss G: -1.6569\n",
      "Epoch [2/30] Batch 200/1250                Loss D: -0.0461,loss G: -1.9108\n",
      "Epoch [2/30] Batch 300/1250                Loss D: 0.0042,loss G: -2.6254\n",
      "Epoch [2/30] Batch 400/1250                Loss D: -0.0098,loss G: -2.2088\n",
      "Epoch [2/30] Batch 500/1250                Loss D: -0.0243,loss G: -2.1552\n",
      "Epoch [2/30] Batch 600/1250                Loss D: -0.0378,loss G: -1.7973\n",
      "Epoch [2/30] Batch 700/1250                Loss D: -0.0636,loss G: -2.0743\n",
      "Epoch [2/30] Batch 800/1250                Loss D: -0.0258,loss G: -1.8714\n",
      "Epoch [2/30] Batch 900/1250                Loss D: -0.0120,loss G: -2.1998\n",
      "Epoch [2/30] Batch 1000/1250                Loss D: -0.0194,loss G: -2.6385\n",
      "Epoch [2/30] Batch 1100/1250                Loss D: -0.0014,loss G: -2.6371\n",
      "Epoch [2/30] Batch 1200/1250                Loss D: -0.0026,loss G: -2.2474\n",
      "Epoch [3/30] Batch 0/1250                Loss D: -0.0272,loss G: -2.3203\n",
      "Epoch [3/30] Batch 100/1250                Loss D: -0.0278,loss G: -2.3783\n",
      "Epoch [3/30] Batch 200/1250                Loss D: -0.0403,loss G: -2.4251\n",
      "Epoch [3/30] Batch 300/1250                Loss D: -0.0687,loss G: -1.6727\n",
      "Epoch [3/30] Batch 400/1250                Loss D: -0.0406,loss G: -1.8121\n",
      "Epoch [3/30] Batch 500/1250                Loss D: -0.0479,loss G: -2.5902\n",
      "Epoch [3/30] Batch 600/1250                Loss D: -0.0135,loss G: -1.9189\n",
      "Epoch [3/30] Batch 700/1250                Loss D: -0.0102,loss G: -2.9425\n",
      "Epoch [3/30] Batch 800/1250                Loss D: -0.0554,loss G: -3.0279\n",
      "Epoch [3/30] Batch 900/1250                Loss D: 0.0161,loss G: -2.9014\n",
      "Epoch [3/30] Batch 1000/1250                Loss D: -0.0355,loss G: -3.5878\n",
      "Epoch [3/30] Batch 1100/1250                Loss D: 0.0041,loss G: -3.4323\n",
      "Epoch [3/30] Batch 1200/1250                Loss D: -0.0375,loss G: -3.5435\n",
      "Epoch [4/30] Batch 0/1250                Loss D: -0.0325,loss G: -3.2990\n",
      "Epoch [4/30] Batch 100/1250                Loss D: -0.0447,loss G: -4.2073\n",
      "Epoch [4/30] Batch 200/1250                Loss D: -0.0082,loss G: -4.1762\n",
      "Epoch [4/30] Batch 300/1250                Loss D: -0.0451,loss G: -3.9972\n",
      "Epoch [4/30] Batch 400/1250                Loss D: -0.0149,loss G: -3.5618\n",
      "Epoch [4/30] Batch 500/1250                Loss D: -0.0230,loss G: -3.5211\n",
      "Epoch [4/30] Batch 600/1250                Loss D: -0.0340,loss G: -3.8975\n",
      "Epoch [4/30] Batch 700/1250                Loss D: -0.0029,loss G: -3.0735\n",
      "Epoch [4/30] Batch 800/1250                Loss D: -0.0336,loss G: -4.1796\n",
      "Epoch [4/30] Batch 900/1250                Loss D: -0.0073,loss G: -3.6973\n",
      "Epoch [4/30] Batch 1000/1250                Loss D: 0.0099,loss G: -4.2969\n",
      "Epoch [4/30] Batch 1100/1250                Loss D: -0.0295,loss G: -4.1061\n",
      "Epoch [4/30] Batch 1200/1250                Loss D: -0.0483,loss G: -4.8988\n",
      "Epoch [5/30] Batch 0/1250                Loss D: -0.0663,loss G: -4.8778\n",
      "Epoch [5/30] Batch 100/1250                Loss D: -0.0436,loss G: -3.6823\n",
      "Epoch [5/30] Batch 200/1250                Loss D: -0.0383,loss G: -3.7873\n",
      "Epoch [5/30] Batch 300/1250                Loss D: -0.0079,loss G: -4.4721\n",
      "Epoch [5/30] Batch 400/1250                Loss D: 0.0004,loss G: -3.7792\n",
      "Epoch [5/30] Batch 500/1250                Loss D: -0.0260,loss G: -4.2226\n",
      "Epoch [5/30] Batch 600/1250                Loss D: -0.0673,loss G: -3.7913\n",
      "Epoch [5/30] Batch 700/1250                Loss D: -0.0251,loss G: -4.0690\n",
      "Epoch [5/30] Batch 800/1250                Loss D: -0.0655,loss G: -3.6791\n",
      "Epoch [5/30] Batch 900/1250                Loss D: -0.0364,loss G: -4.0028\n",
      "Epoch [5/30] Batch 1000/1250                Loss D: 0.0017,loss G: -3.4857\n",
      "Epoch [5/30] Batch 1100/1250                Loss D: 0.0339,loss G: -3.7948\n",
      "Epoch [5/30] Batch 1200/1250                Loss D: -0.0280,loss G: -4.3324\n",
      "Epoch [6/30] Batch 0/1250                Loss D: 0.0091,loss G: -4.2434\n",
      "Epoch [6/30] Batch 100/1250                Loss D: -0.0320,loss G: -3.8478\n",
      "Epoch [6/30] Batch 200/1250                Loss D: -0.0127,loss G: -3.8711\n",
      "Epoch [6/30] Batch 300/1250                Loss D: -0.0435,loss G: -4.6168\n",
      "Epoch [6/30] Batch 400/1250                Loss D: -0.0095,loss G: -4.1160\n",
      "Epoch [6/30] Batch 500/1250                Loss D: -0.0668,loss G: -4.1364\n",
      "Epoch [6/30] Batch 600/1250                Loss D: -0.0202,loss G: -3.6381\n",
      "Epoch [6/30] Batch 700/1250                Loss D: -0.0005,loss G: -3.2631\n",
      "Epoch [6/30] Batch 800/1250                Loss D: -0.0157,loss G: -3.9776\n",
      "Epoch [6/30] Batch 900/1250                Loss D: -0.0622,loss G: -4.0229\n",
      "Epoch [6/30] Batch 1000/1250                Loss D: -0.0301,loss G: -3.8776\n",
      "Epoch [6/30] Batch 1100/1250                Loss D: -0.0154,loss G: -4.2314\n",
      "Epoch [6/30] Batch 1200/1250                Loss D: -0.0659,loss G: -3.4959\n",
      "Epoch [7/30] Batch 0/1250                Loss D: -0.0088,loss G: -5.2085\n",
      "Epoch [7/30] Batch 100/1250                Loss D: -0.0442,loss G: -4.0571\n",
      "Epoch [7/30] Batch 200/1250                Loss D: 0.0021,loss G: -3.7998\n",
      "Epoch [7/30] Batch 300/1250                Loss D: -0.0433,loss G: -4.4106\n",
      "Epoch [7/30] Batch 400/1250                Loss D: -0.0319,loss G: -5.0880\n",
      "Epoch [7/30] Batch 500/1250                Loss D: -0.0212,loss G: -4.5156\n",
      "Epoch [7/30] Batch 600/1250                Loss D: 0.0010,loss G: -4.8570\n",
      "Epoch [7/30] Batch 700/1250                Loss D: 0.0965,loss G: -4.7345\n",
      "Epoch [7/30] Batch 800/1250                Loss D: -0.0056,loss G: -5.7291\n",
      "Epoch [7/30] Batch 900/1250                Loss D: -0.0213,loss G: -5.9292\n",
      "Epoch [7/30] Batch 1000/1250                Loss D: 0.0057,loss G: -6.4183\n",
      "Epoch [7/30] Batch 1100/1250                Loss D: -0.0326,loss G: -6.1389\n",
      "Epoch [7/30] Batch 1200/1250                Loss D: -0.0182,loss G: -6.2488\n",
      "Epoch [8/30] Batch 0/1250                Loss D: -0.0155,loss G: -6.9487\n",
      "Epoch [8/30] Batch 100/1250                Loss D: -0.0307,loss G: -6.0693\n",
      "Epoch [8/30] Batch 200/1250                Loss D: -0.0203,loss G: -6.4425\n",
      "Epoch [8/30] Batch 300/1250                Loss D: -0.0145,loss G: -5.6509\n",
      "Epoch [8/30] Batch 400/1250                Loss D: -0.0053,loss G: -6.2007\n",
      "Epoch [8/30] Batch 500/1250                Loss D: -0.0268,loss G: -5.9832\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8/30] Batch 600/1250                Loss D: -0.0218,loss G: -5.6562\n",
      "Epoch [8/30] Batch 700/1250                Loss D: -0.0117,loss G: -5.9799\n",
      "Epoch [8/30] Batch 800/1250                Loss D: -0.0469,loss G: -5.0836\n",
      "Epoch [8/30] Batch 900/1250                Loss D: 0.0032,loss G: -5.6267\n",
      "Epoch [8/30] Batch 1000/1250                Loss D: 0.0047,loss G: -6.0664\n",
      "Epoch [8/30] Batch 1100/1250                Loss D: -0.0371,loss G: -5.5047\n",
      "Epoch [8/30] Batch 1200/1250                Loss D: -0.0052,loss G: -5.7989\n",
      "Epoch [9/30] Batch 0/1250                Loss D: -0.0644,loss G: -5.4274\n",
      "Epoch [9/30] Batch 100/1250                Loss D: 0.0103,loss G: -5.6897\n",
      "Epoch [9/30] Batch 200/1250                Loss D: -0.0543,loss G: -6.6217\n",
      "Epoch [9/30] Batch 300/1250                Loss D: -0.0388,loss G: -6.3422\n",
      "Epoch [9/30] Batch 400/1250                Loss D: -0.0272,loss G: -6.3342\n",
      "Epoch [9/30] Batch 500/1250                Loss D: -0.0259,loss G: -6.6392\n",
      "Epoch [9/30] Batch 600/1250                Loss D: -0.0110,loss G: -6.4419\n",
      "Epoch [9/30] Batch 700/1250                Loss D: -0.0200,loss G: -7.0092\n",
      "Epoch [9/30] Batch 800/1250                Loss D: 0.0149,loss G: -7.5961\n",
      "Epoch [9/30] Batch 900/1250                Loss D: -0.0221,loss G: -7.6289\n",
      "Epoch [9/30] Batch 1000/1250                Loss D: -0.0462,loss G: -7.3264\n",
      "Epoch [9/30] Batch 1100/1250                Loss D: -0.0437,loss G: -7.5116\n",
      "Epoch [9/30] Batch 1200/1250                Loss D: -0.0257,loss G: -7.5805\n",
      "Epoch [10/30] Batch 0/1250                Loss D: -0.0079,loss G: -7.4044\n",
      "Epoch [10/30] Batch 100/1250                Loss D: -0.0418,loss G: -7.4792\n",
      "Epoch [10/30] Batch 200/1250                Loss D: -0.0565,loss G: -6.5772\n",
      "Epoch [10/30] Batch 300/1250                Loss D: -0.0388,loss G: -8.1831\n",
      "Epoch [10/30] Batch 400/1250                Loss D: 0.0167,loss G: -7.9267\n",
      "Epoch [10/30] Batch 500/1250                Loss D: -0.0396,loss G: -7.3185\n",
      "Epoch [10/30] Batch 600/1250                Loss D: 0.0117,loss G: -8.8812\n",
      "Epoch [10/30] Batch 700/1250                Loss D: -0.0069,loss G: -7.7990\n",
      "Epoch [10/30] Batch 800/1250                Loss D: -0.0368,loss G: -7.8626\n",
      "Epoch [10/30] Batch 900/1250                Loss D: -0.0144,loss G: -7.7497\n",
      "Epoch [10/30] Batch 1000/1250                Loss D: -0.0029,loss G: -7.8190\n",
      "Epoch [10/30] Batch 1100/1250                Loss D: -0.0135,loss G: -7.7738\n",
      "Epoch [10/30] Batch 1200/1250                Loss D: 0.0009,loss G: -8.1148\n",
      "Epoch [11/30] Batch 0/1250                Loss D: -0.0274,loss G: -7.7906\n",
      "Epoch [11/30] Batch 100/1250                Loss D: -0.0395,loss G: -8.0833\n",
      "Epoch [11/30] Batch 200/1250                Loss D: 0.0064,loss G: -7.1596\n",
      "Epoch [11/30] Batch 300/1250                Loss D: -0.0023,loss G: -7.2993\n",
      "Epoch [11/30] Batch 400/1250                Loss D: -0.0072,loss G: -8.6221\n",
      "Epoch [11/30] Batch 500/1250                Loss D: 0.0220,loss G: -7.0588\n",
      "Epoch [11/30] Batch 600/1250                Loss D: 0.0088,loss G: -7.3316\n",
      "Epoch [11/30] Batch 700/1250                Loss D: -0.0407,loss G: -6.6356\n",
      "Epoch [11/30] Batch 800/1250                Loss D: -0.0219,loss G: -7.4950\n",
      "Epoch [11/30] Batch 900/1250                Loss D: -0.0042,loss G: -7.1299\n",
      "Epoch [11/30] Batch 1000/1250                Loss D: -0.0433,loss G: -6.8850\n",
      "Epoch [11/30] Batch 1100/1250                Loss D: -0.0217,loss G: -7.6591\n",
      "Epoch [11/30] Batch 1200/1250                Loss D: -0.0122,loss G: -7.3996\n",
      "Epoch [12/30] Batch 0/1250                Loss D: -0.0114,loss G: -7.2892\n",
      "Epoch [12/30] Batch 100/1250                Loss D: -0.0197,loss G: -8.2175\n",
      "Epoch [12/30] Batch 200/1250                Loss D: 0.0034,loss G: -8.2403\n",
      "Epoch [12/30] Batch 300/1250                Loss D: -0.0389,loss G: -7.5458\n",
      "Epoch [12/30] Batch 400/1250                Loss D: -0.0140,loss G: -8.2962\n",
      "Epoch [12/30] Batch 500/1250                Loss D: -0.0164,loss G: -7.8766\n",
      "Epoch [12/30] Batch 600/1250                Loss D: -0.0237,loss G: -8.5841\n",
      "Epoch [12/30] Batch 700/1250                Loss D: -0.0471,loss G: -8.0819\n",
      "Epoch [12/30] Batch 800/1250                Loss D: -0.0377,loss G: -8.3415\n",
      "Epoch [12/30] Batch 900/1250                Loss D: -0.0140,loss G: -8.3430\n",
      "Epoch [12/30] Batch 1000/1250                Loss D: -0.0337,loss G: -7.8188\n",
      "Epoch [12/30] Batch 1100/1250                Loss D: -0.0264,loss G: -8.4861\n",
      "Epoch [12/30] Batch 1200/1250                Loss D: -0.0216,loss G: -8.2841\n",
      "Epoch [13/30] Batch 0/1250                Loss D: -0.0097,loss G: -8.4455\n",
      "Epoch [13/30] Batch 100/1250                Loss D: -0.0063,loss G: -8.7698\n",
      "Epoch [13/30] Batch 200/1250                Loss D: -0.0016,loss G: -7.7454\n",
      "Epoch [13/30] Batch 300/1250                Loss D: 0.0010,loss G: -8.7164\n",
      "Epoch [13/30] Batch 400/1250                Loss D: 0.0552,loss G: -9.2831\n",
      "Epoch [13/30] Batch 500/1250                Loss D: -0.0016,loss G: -7.2796\n",
      "Epoch [13/30] Batch 600/1250                Loss D: 0.0257,loss G: -7.5759\n",
      "Epoch [13/30] Batch 700/1250                Loss D: -0.0105,loss G: -8.4085\n",
      "Epoch [13/30] Batch 800/1250                Loss D: -0.0105,loss G: -9.3302\n",
      "Epoch [13/30] Batch 900/1250                Loss D: -0.0249,loss G: -10.6526\n",
      "Epoch [13/30] Batch 1000/1250                Loss D: -0.0277,loss G: -10.4481\n",
      "Epoch [13/30] Batch 1100/1250                Loss D: -0.0162,loss G: -9.3113\n",
      "Epoch [13/30] Batch 1200/1250                Loss D: -0.0504,loss G: -9.6767\n",
      "Epoch [14/30] Batch 0/1250                Loss D: -0.0261,loss G: -8.6224\n",
      "Epoch [14/30] Batch 100/1250                Loss D: 0.0080,loss G: -8.1676\n",
      "Epoch [14/30] Batch 200/1250                Loss D: -0.0587,loss G: -8.7524\n",
      "Epoch [14/30] Batch 300/1250                Loss D: -0.0294,loss G: -8.9199\n",
      "Epoch [14/30] Batch 400/1250                Loss D: -0.0410,loss G: -8.2819\n",
      "Epoch [14/30] Batch 500/1250                Loss D: -0.0154,loss G: -8.3872\n",
      "Epoch [14/30] Batch 600/1250                Loss D: -0.0281,loss G: -8.6223\n",
      "Epoch [14/30] Batch 700/1250                Loss D: 0.0045,loss G: -9.4449\n",
      "Epoch [14/30] Batch 800/1250                Loss D: -0.0116,loss G: -8.0772\n",
      "Epoch [14/30] Batch 900/1250                Loss D: -0.0345,loss G: -8.6622\n",
      "Epoch [14/30] Batch 1000/1250                Loss D: -0.0069,loss G: -9.6271\n",
      "Epoch [14/30] Batch 1100/1250                Loss D: -0.0331,loss G: -8.8806\n",
      "Epoch [14/30] Batch 1200/1250                Loss D: 0.0011,loss G: -9.3093\n",
      "Epoch [15/30] Batch 0/1250                Loss D: -0.0281,loss G: -9.0346\n",
      "Epoch [15/30] Batch 100/1250                Loss D: -0.0217,loss G: -10.3869\n",
      "Epoch [15/30] Batch 200/1250                Loss D: -0.0159,loss G: -10.3883\n",
      "Epoch [15/30] Batch 300/1250                Loss D: 0.0036,loss G: -9.2133\n",
      "Epoch [15/30] Batch 400/1250                Loss D: -0.0072,loss G: -9.5544\n",
      "Epoch [15/30] Batch 500/1250                Loss D: -0.0236,loss G: -8.8833\n",
      "Epoch [15/30] Batch 600/1250                Loss D: -0.0247,loss G: -9.5117\n",
      "Epoch [15/30] Batch 700/1250                Loss D: -0.0051,loss G: -7.6316\n",
      "Epoch [15/30] Batch 800/1250                Loss D: -0.0090,loss G: -8.9587\n",
      "Epoch [15/30] Batch 900/1250                Loss D: -0.0393,loss G: -8.2464\n",
      "Epoch [15/30] Batch 1000/1250                Loss D: -0.0150,loss G: -8.8911\n",
      "Epoch [15/30] Batch 1100/1250                Loss D: -0.0048,loss G: -9.3326\n",
      "Epoch [15/30] Batch 1200/1250                Loss D: -0.0270,loss G: -9.3108\n",
      "Epoch [16/30] Batch 0/1250                Loss D: -0.0328,loss G: -8.6943\n",
      "Epoch [16/30] Batch 100/1250                Loss D: -0.0590,loss G: -7.9463\n",
      "Epoch [16/30] Batch 200/1250                Loss D: -0.0424,loss G: -8.3011\n",
      "Epoch [16/30] Batch 300/1250                Loss D: -0.0093,loss G: -9.0254\n",
      "Epoch [16/30] Batch 400/1250                Loss D: -0.0475,loss G: -8.9111\n",
      "Epoch [16/30] Batch 500/1250                Loss D: -0.0324,loss G: -8.2341\n",
      "Epoch [16/30] Batch 600/1250                Loss D: -0.0379,loss G: -8.4494\n",
      "Epoch [16/30] Batch 700/1250                Loss D: -0.0503,loss G: -10.0653\n",
      "Epoch [16/30] Batch 800/1250                Loss D: -0.0200,loss G: -9.0533\n",
      "Epoch [16/30] Batch 900/1250                Loss D: -0.0262,loss G: -8.7737\n",
      "Epoch [16/30] Batch 1000/1250                Loss D: 0.0066,loss G: -8.5932\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [16/30] Batch 1100/1250                Loss D: -0.0305,loss G: -10.2047\n",
      "Epoch [16/30] Batch 1200/1250                Loss D: -0.0905,loss G: -9.2802\n",
      "Epoch [17/30] Batch 0/1250                Loss D: -0.0224,loss G: -10.2763\n",
      "Epoch [17/30] Batch 100/1250                Loss D: -0.0405,loss G: -10.0164\n",
      "Epoch [17/30] Batch 200/1250                Loss D: -0.0173,loss G: -9.5704\n",
      "Epoch [17/30] Batch 300/1250                Loss D: -0.0125,loss G: -10.5101\n",
      "Epoch [17/30] Batch 400/1250                Loss D: -0.0242,loss G: -10.2340\n",
      "Epoch [17/30] Batch 500/1250                Loss D: -0.0180,loss G: -11.4145\n",
      "Epoch [17/30] Batch 600/1250                Loss D: 0.0149,loss G: -12.4856\n",
      "Epoch [17/30] Batch 700/1250                Loss D: -0.0306,loss G: -11.1520\n",
      "Epoch [17/30] Batch 800/1250                Loss D: -0.0052,loss G: -11.3938\n",
      "Epoch [17/30] Batch 900/1250                Loss D: -0.0324,loss G: -11.6689\n",
      "Epoch [17/30] Batch 1000/1250                Loss D: -0.0451,loss G: -10.9389\n",
      "Epoch [17/30] Batch 1100/1250                Loss D: -0.0186,loss G: -9.9409\n",
      "Epoch [17/30] Batch 1200/1250                Loss D: -0.0330,loss G: -10.2774\n",
      "Epoch [18/30] Batch 0/1250                Loss D: -0.0357,loss G: -11.8011\n",
      "Epoch [18/30] Batch 100/1250                Loss D: -0.0266,loss G: -12.4736\n",
      "Epoch [18/30] Batch 200/1250                Loss D: -0.0378,loss G: -12.4310\n",
      "Epoch [18/30] Batch 300/1250                Loss D: -0.0284,loss G: -12.5468\n",
      "Epoch [18/30] Batch 400/1250                Loss D: 0.0249,loss G: -11.8737\n",
      "Epoch [18/30] Batch 500/1250                Loss D: -0.0344,loss G: -12.4071\n",
      "Epoch [18/30] Batch 600/1250                Loss D: -0.0167,loss G: -11.6661\n",
      "Epoch [18/30] Batch 700/1250                Loss D: -0.0502,loss G: -11.4722\n",
      "Epoch [18/30] Batch 800/1250                Loss D: -0.0502,loss G: -11.2971\n",
      "Epoch [18/30] Batch 900/1250                Loss D: -0.0124,loss G: -11.8822\n",
      "Epoch [18/30] Batch 1000/1250                Loss D: -0.0287,loss G: -11.6463\n",
      "Epoch [18/30] Batch 1100/1250                Loss D: -0.0346,loss G: -10.9102\n",
      "Epoch [18/30] Batch 1200/1250                Loss D: -0.0281,loss G: -9.7698\n",
      "Epoch [19/30] Batch 0/1250                Loss D: -0.0340,loss G: -10.9104\n",
      "Epoch [19/30] Batch 100/1250                Loss D: -0.0241,loss G: -10.0122\n",
      "Epoch [19/30] Batch 200/1250                Loss D: -0.0813,loss G: -10.2348\n",
      "Epoch [19/30] Batch 300/1250                Loss D: -0.0078,loss G: -9.4685\n",
      "Epoch [19/30] Batch 400/1250                Loss D: -0.0351,loss G: -9.7736\n",
      "Epoch [19/30] Batch 500/1250                Loss D: -0.0039,loss G: -9.7380\n",
      "Epoch [19/30] Batch 600/1250                Loss D: -0.0280,loss G: -11.5554\n",
      "Epoch [19/30] Batch 700/1250                Loss D: -0.0405,loss G: -10.6231\n",
      "Epoch [19/30] Batch 800/1250                Loss D: -0.0263,loss G: -9.5668\n",
      "Epoch [19/30] Batch 900/1250                Loss D: -0.0036,loss G: -9.6732\n",
      "Epoch [19/30] Batch 1000/1250                Loss D: -0.0286,loss G: -11.2014\n",
      "Epoch [19/30] Batch 1100/1250                Loss D: -0.0297,loss G: -10.6400\n",
      "Epoch [19/30] Batch 1200/1250                Loss D: -0.0368,loss G: -11.2452\n",
      "Epoch [20/30] Batch 0/1250                Loss D: -0.0688,loss G: -11.0921\n",
      "Epoch [20/30] Batch 100/1250                Loss D: 0.0079,loss G: -11.5131\n",
      "Epoch [20/30] Batch 200/1250                Loss D: -0.0156,loss G: -10.9030\n",
      "Epoch [20/30] Batch 300/1250                Loss D: -0.0206,loss G: -11.0604\n",
      "Epoch [20/30] Batch 400/1250                Loss D: -0.0073,loss G: -12.3729\n",
      "Epoch [20/30] Batch 500/1250                Loss D: -0.0266,loss G: -12.5850\n",
      "Epoch [20/30] Batch 600/1250                Loss D: 0.0064,loss G: -11.9392\n",
      "Epoch [20/30] Batch 700/1250                Loss D: -0.0106,loss G: -12.7163\n",
      "Epoch [20/30] Batch 800/1250                Loss D: -0.0610,loss G: -13.1947\n",
      "Epoch [20/30] Batch 900/1250                Loss D: -0.0221,loss G: -13.2882\n",
      "Epoch [20/30] Batch 1000/1250                Loss D: -0.0075,loss G: -12.6697\n",
      "Epoch [20/30] Batch 1100/1250                Loss D: -0.0376,loss G: -12.7597\n",
      "Epoch [20/30] Batch 1200/1250                Loss D: 0.0142,loss G: -12.7935\n",
      "Epoch [21/30] Batch 0/1250                Loss D: -0.0677,loss G: -12.0801\n",
      "Epoch [21/30] Batch 100/1250                Loss D: -0.0140,loss G: -13.0044\n",
      "Epoch [21/30] Batch 200/1250                Loss D: -0.0116,loss G: -12.7722\n",
      "Epoch [21/30] Batch 300/1250                Loss D: -0.0433,loss G: -12.8279\n",
      "Epoch [21/30] Batch 400/1250                Loss D: -0.0056,loss G: -13.2225\n",
      "Epoch [21/30] Batch 500/1250                Loss D: -0.0280,loss G: -13.5917\n",
      "Epoch [21/30] Batch 600/1250                Loss D: -0.0120,loss G: -14.2689\n",
      "Epoch [21/30] Batch 700/1250                Loss D: 0.0211,loss G: -14.5164\n",
      "Epoch [21/30] Batch 800/1250                Loss D: -0.0151,loss G: -13.5520\n",
      "Epoch [21/30] Batch 900/1250                Loss D: -0.0533,loss G: -12.6496\n",
      "Epoch [21/30] Batch 1000/1250                Loss D: -0.0259,loss G: -14.4435\n",
      "Epoch [21/30] Batch 1100/1250                Loss D: -0.0557,loss G: -13.8989\n",
      "Epoch [21/30] Batch 1200/1250                Loss D: -0.0472,loss G: -14.2708\n",
      "Epoch [22/30] Batch 0/1250                Loss D: -0.0018,loss G: -12.9746\n",
      "Epoch [22/30] Batch 100/1250                Loss D: -0.0350,loss G: -13.5169\n",
      "Epoch [22/30] Batch 200/1250                Loss D: -0.0678,loss G: -14.0221\n",
      "Epoch [22/30] Batch 300/1250                Loss D: -0.0174,loss G: -14.2197\n",
      "Epoch [22/30] Batch 400/1250                Loss D: -0.0210,loss G: -14.2697\n",
      "Epoch [22/30] Batch 500/1250                Loss D: -0.0020,loss G: -13.9453\n",
      "Epoch [22/30] Batch 600/1250                Loss D: -0.0495,loss G: -14.6573\n",
      "Epoch [22/30] Batch 700/1250                Loss D: 0.0185,loss G: -15.1738\n",
      "Epoch [22/30] Batch 800/1250                Loss D: 0.0136,loss G: -16.0541\n",
      "Epoch [22/30] Batch 900/1250                Loss D: 0.0282,loss G: -15.4006\n",
      "Epoch [22/30] Batch 1000/1250                Loss D: -0.0610,loss G: -13.2350\n",
      "Epoch [22/30] Batch 1100/1250                Loss D: 0.0065,loss G: -13.7804\n",
      "Epoch [22/30] Batch 1200/1250                Loss D: 0.0147,loss G: -13.2196\n",
      "Epoch [23/30] Batch 0/1250                Loss D: -0.0347,loss G: -13.4994\n",
      "Epoch [23/30] Batch 100/1250                Loss D: -0.0064,loss G: -13.9007\n",
      "Epoch [23/30] Batch 200/1250                Loss D: -0.0227,loss G: -14.8000\n",
      "Epoch [23/30] Batch 300/1250                Loss D: 0.0217,loss G: -14.7576\n",
      "Epoch [23/30] Batch 400/1250                Loss D: -0.0386,loss G: -14.4476\n",
      "Epoch [23/30] Batch 500/1250                Loss D: -0.0081,loss G: -14.2159\n",
      "Epoch [23/30] Batch 600/1250                Loss D: -0.0113,loss G: -15.5523\n",
      "Epoch [23/30] Batch 700/1250                Loss D: -0.0499,loss G: -14.5127\n",
      "Epoch [23/30] Batch 800/1250                Loss D: -0.0253,loss G: -15.0292\n",
      "Epoch [23/30] Batch 900/1250                Loss D: -0.0140,loss G: -15.1608\n",
      "Epoch [23/30] Batch 1000/1250                Loss D: -0.0679,loss G: -14.7329\n",
      "Epoch [23/30] Batch 1100/1250                Loss D: -0.0679,loss G: -13.8696\n",
      "Epoch [23/30] Batch 1200/1250                Loss D: 0.0157,loss G: -15.1210\n",
      "Epoch [24/30] Batch 0/1250                Loss D: -0.0366,loss G: -15.7292\n",
      "Epoch [24/30] Batch 100/1250                Loss D: 0.0281,loss G: -16.6647\n",
      "Epoch [24/30] Batch 200/1250                Loss D: -0.0399,loss G: -15.8326\n",
      "Epoch [24/30] Batch 300/1250                Loss D: -0.0350,loss G: -16.3030\n",
      "Epoch [24/30] Batch 400/1250                Loss D: 0.0162,loss G: -18.7211\n",
      "Epoch [24/30] Batch 500/1250                Loss D: -0.0338,loss G: -17.5767\n",
      "Epoch [24/30] Batch 600/1250                Loss D: -0.0406,loss G: -16.9747\n",
      "Epoch [24/30] Batch 700/1250                Loss D: -0.0255,loss G: -16.5034\n",
      "Epoch [24/30] Batch 800/1250                Loss D: -0.0083,loss G: -15.5307\n",
      "Epoch [24/30] Batch 900/1250                Loss D: -0.0053,loss G: -14.5968\n",
      "Epoch [24/30] Batch 1000/1250                Loss D: -0.0324,loss G: -15.3718\n",
      "Epoch [24/30] Batch 1100/1250                Loss D: -0.0111,loss G: -14.2458\n",
      "Epoch [24/30] Batch 1200/1250                Loss D: -0.0241,loss G: -15.8551\n",
      "Epoch [25/30] Batch 0/1250                Loss D: 0.0031,loss G: -15.9860\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [25/30] Batch 100/1250                Loss D: 0.0519,loss G: -16.5800\n",
      "Epoch [25/30] Batch 200/1250                Loss D: -0.0065,loss G: -15.7906\n",
      "Epoch [25/30] Batch 300/1250                Loss D: -0.0636,loss G: -15.1901\n",
      "Epoch [25/30] Batch 400/1250                Loss D: -0.0523,loss G: -15.7195\n",
      "Epoch [25/30] Batch 500/1250                Loss D: -0.0051,loss G: -15.4518\n",
      "Epoch [25/30] Batch 600/1250                Loss D: -0.0184,loss G: -16.5264\n",
      "Epoch [25/30] Batch 700/1250                Loss D: -0.0310,loss G: -15.3931\n",
      "Epoch [25/30] Batch 800/1250                Loss D: 0.0114,loss G: -14.3057\n",
      "Epoch [25/30] Batch 900/1250                Loss D: 0.0002,loss G: -13.9564\n",
      "Epoch [25/30] Batch 1000/1250                Loss D: -0.0489,loss G: -15.7711\n",
      "Epoch [25/30] Batch 1100/1250                Loss D: -0.0245,loss G: -14.8887\n",
      "Epoch [25/30] Batch 1200/1250                Loss D: -0.0130,loss G: -14.4531\n",
      "Epoch [26/30] Batch 0/1250                Loss D: 0.0131,loss G: -15.0216\n",
      "Epoch [26/30] Batch 100/1250                Loss D: -0.0138,loss G: -16.4107\n",
      "Epoch [26/30] Batch 200/1250                Loss D: -0.0445,loss G: -18.0172\n",
      "Epoch [26/30] Batch 300/1250                Loss D: 0.0343,loss G: -16.7925\n",
      "Epoch [26/30] Batch 400/1250                Loss D: -0.0486,loss G: -18.1940\n",
      "Epoch [26/30] Batch 500/1250                Loss D: -0.0429,loss G: -16.0770\n",
      "Epoch [26/30] Batch 600/1250                Loss D: -0.0313,loss G: -16.9129\n",
      "Epoch [26/30] Batch 700/1250                Loss D: 0.0151,loss G: -15.8178\n",
      "Epoch [26/30] Batch 800/1250                Loss D: -0.0215,loss G: -17.2314\n",
      "Epoch [26/30] Batch 900/1250                Loss D: -0.0210,loss G: -16.3923\n",
      "Epoch [26/30] Batch 1000/1250                Loss D: -0.0308,loss G: -16.0669\n",
      "Epoch [26/30] Batch 1100/1250                Loss D: -0.0400,loss G: -15.9730\n",
      "Epoch [26/30] Batch 1200/1250                Loss D: -0.0604,loss G: -16.2077\n",
      "Epoch [27/30] Batch 0/1250                Loss D: 0.0182,loss G: -16.0227\n",
      "Epoch [27/30] Batch 100/1250                Loss D: -0.0431,loss G: -15.4670\n",
      "Epoch [27/30] Batch 200/1250                Loss D: -0.0373,loss G: -17.0546\n",
      "Epoch [27/30] Batch 300/1250                Loss D: -0.0295,loss G: -16.8232\n",
      "Epoch [27/30] Batch 400/1250                Loss D: -0.0474,loss G: -16.4180\n",
      "Epoch [27/30] Batch 500/1250                Loss D: -0.0138,loss G: -16.8156\n",
      "Epoch [27/30] Batch 600/1250                Loss D: -0.0158,loss G: -16.5683\n",
      "Epoch [27/30] Batch 700/1250                Loss D: -0.0014,loss G: -17.2456\n",
      "Epoch [27/30] Batch 800/1250                Loss D: -0.0724,loss G: -17.4272\n",
      "Epoch [27/30] Batch 900/1250                Loss D: -0.0260,loss G: -16.4238\n",
      "Epoch [27/30] Batch 1000/1250                Loss D: -0.0583,loss G: -15.7220\n",
      "Epoch [27/30] Batch 1100/1250                Loss D: 0.0096,loss G: -16.1284\n",
      "Epoch [27/30] Batch 1200/1250                Loss D: -0.0520,loss G: -17.2010\n",
      "Epoch [28/30] Batch 0/1250                Loss D: -0.0392,loss G: -15.9442\n",
      "Epoch [28/30] Batch 100/1250                Loss D: -0.0227,loss G: -15.9440\n",
      "Epoch [28/30] Batch 200/1250                Loss D: -0.0519,loss G: -16.0743\n",
      "Epoch [28/30] Batch 300/1250                Loss D: -0.0225,loss G: -15.6678\n",
      "Epoch [28/30] Batch 400/1250                Loss D: -0.0165,loss G: -16.0010\n",
      "Epoch [28/30] Batch 500/1250                Loss D: -0.0469,loss G: -17.3929\n",
      "Epoch [28/30] Batch 600/1250                Loss D: -0.0466,loss G: -17.3148\n",
      "Epoch [28/30] Batch 700/1250                Loss D: -0.0221,loss G: -16.5891\n",
      "Epoch [28/30] Batch 800/1250                Loss D: -0.0227,loss G: -17.2342\n",
      "Epoch [28/30] Batch 900/1250                Loss D: -0.0364,loss G: -16.5800\n",
      "Epoch [28/30] Batch 1000/1250                Loss D: 0.0038,loss G: -16.3799\n",
      "Epoch [28/30] Batch 1100/1250                Loss D: -0.0029,loss G: -16.2165\n",
      "Epoch [28/30] Batch 1200/1250                Loss D: 0.0078,loss G: -16.3242\n",
      "Epoch [29/30] Batch 0/1250                Loss D: -0.0332,loss G: -16.7625\n",
      "Epoch [29/30] Batch 100/1250                Loss D: -0.0301,loss G: -18.0797\n",
      "Epoch [29/30] Batch 200/1250                Loss D: -0.0193,loss G: -17.8595\n",
      "Epoch [29/30] Batch 300/1250                Loss D: -0.0208,loss G: -19.0203\n",
      "Epoch [29/30] Batch 400/1250                Loss D: 0.0021,loss G: -16.5391\n",
      "Epoch [29/30] Batch 500/1250                Loss D: 0.0167,loss G: -19.7974\n",
      "Epoch [29/30] Batch 600/1250                Loss D: 0.0083,loss G: -19.5730\n",
      "Epoch [29/30] Batch 700/1250                Loss D: -0.0717,loss G: -20.7431\n",
      "Epoch [29/30] Batch 800/1250                Loss D: -0.0396,loss G: -18.5680\n",
      "Epoch [29/30] Batch 900/1250                Loss D: -0.0305,loss G: -17.0618\n",
      "Epoch [29/30] Batch 1000/1250                Loss D: -0.0224,loss G: -17.4119\n",
      "Epoch [29/30] Batch 1100/1250                Loss D: -0.0396,loss G: -17.5795\n",
      "Epoch [29/30] Batch 1200/1250                Loss D: 0.0065,loss G: -17.6754\n"
     ]
    }
   ],
   "source": [
    "gen.train()\n",
    "critic.train()\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "     \n",
    "    for batch_idx,data in enumerate(loader):\n",
    "        real = data[1]\n",
    "        real = real.to(device)\n",
    "        cur_batch_size = real.shape[0]\n",
    "        labels = data[0]\n",
    "        labels = labels.to(device)\n",
    "#     for batch_idx, (real,labels) in enumerate(loader):\n",
    "#         real = real.to(device)\n",
    "#         cur_batch_size = real.shape[0]\n",
    "#         labels = labels.to(device)\n",
    "        \n",
    "        for _ in range(CRITIC_ITERATIONS):\n",
    "            noise = torch.randn((cur_batch_size,Z_DIM,1,1)).to(device)\n",
    "            fake = gen(noise,labels)\n",
    "            critic_real = critic(real,labels).reshape(-1)\n",
    "            critic_fake = critic(fake,labels).reshape(-1)\n",
    "            gp = gradient_penalty(critic,labels,real,fake,device=device)\n",
    "            loss_critic = ( -(torch.mean(critic_real)-torch.mean(critic_fake)) +LAMBDA_GP*gp)\n",
    "            critic.zero_grad()\n",
    "            loss_critic.backward(retain_graph = True)\n",
    "            opt_critic.step()\n",
    "     \n",
    "        \n",
    "        ###训练生成器 min log(1-D(G(z))) <----->max log(D(G(z))) \n",
    "        gen_fake = critic(fake,labels).reshape(-1)\n",
    "        loss_gen = -torch.mean(gen_fake)        \n",
    "        gen.zero_grad()\n",
    "        loss_gen.backward()\n",
    "        opt_gen.step()\n",
    "        \n",
    "        \n",
    "        # Print losses osccasionally and print to tensorboard\n",
    "        if batch_idx %100==0:\n",
    "            print(\n",
    "                f\"Epoch [{epoch}/{NUM_EPOCHS}] Batch {batch_idx}/{len(loader)}\\\n",
    "                Loss D: {loss_critic:.4f},loss G: {loss_gen:.4f}\"\n",
    "            )\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                fake = gen(noise,labels)\n",
    "                #take out(up to) 32 examples\n",
    "                img_grid_real = torchvision.utils.make_grid(\n",
    "                    real[:32],normalize = True\n",
    "                \n",
    "                )\n",
    "                \n",
    "                img_grid_fake = torchvision.utils.make_grid(\n",
    "                    fake[:32],normalize = True\n",
    "                \n",
    "                )\n",
    "                \n",
    "                writer_real.add_image(\"Real\",img_grid_real,global_step=step)\n",
    "                writer_fake.add_image(\"Fake\",img_grid_fake,global_step=step)\n",
    "                \n",
    "            step +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(gen.state_dict(),'./output/WCGAN_gen.pth')\n",
    "torch.save(critic.state_dict(),'./output/WCGANcritic.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chechpoint=torch.load('./output/WCGAN_gen.pth')\n",
    "gen = Generator(Z_DIM,CHANNELS_IMG,FEATURES_GEN,NUM_CLASSES,IMG_SIZE_L,IMG_SIZE_H,GEN_EMBEDDING).to(device)\n",
    "gen.load_state_dict(chechpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([30000, 1, 7, 48])\n"
     ]
    }
   ],
   "source": [
    "####Used for generating sythetic data\n",
    "####For the CER Dataset,Question 300 has 3 different labels,we set the input label of generator to control the output class\n",
    "#### Here we generate the data with label 1\n",
    "batch_out_data = 30000\n",
    "out_noise_1 = torch.randn((batch_out_data,100,1,1))\n",
    "out_1 = torch.ones(batch_out_data)\n",
    "out_1 = torch.FloatTensor(out_1).long()\n",
    "out_noise_1=out_noise_1.to(device)\n",
    "out_1 = out_1.to(device)\n",
    "fake1 = gen(out_noise_1,out_1).to(device)\n",
    "print(fake1.shape)\n",
    "L_1 = fake1.view(batch_out_data,7*48)*12.469\n",
    "L_1=L_1.cpu().detach().numpy().tolist()\n",
    "np.savetxt('./Generated_Data_30000/Q300_1.csv',L_1,delimiter=',')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([30000, 1, 7, 48])\n"
     ]
    }
   ],
   "source": [
    "####Used for generating sythetic data\n",
    "####For the CER Dataset,Question 300 has 3 different labels,we set the input label of generator to control the output class\n",
    "#### Here we generate the data with label 2\n",
    "batch_out_data = 30000\n",
    "out_noise_2 = torch.randn((batch_out_data,100,1,1))\n",
    "out_2 = torch.ones(batch_out_data)*2\n",
    "out_2 = torch.FloatTensor(out_2).long()\n",
    "out_noise_2=out_noise_2.to(device)\n",
    "out_2 = out_2.to(device)\n",
    "fake2 = gen(out_noise_2,out_2).to(device)\n",
    "print(fake2.shape)\n",
    "L_2 = fake2.view(batch_out_data,7*48)*12.469\n",
    "L_2=L_2.cpu().detach().numpy().tolist()\n",
    "np.savetxt('./Generated_Data_30000/Q300_2.csv',L_2,delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([30000, 1, 7, 48])\n"
     ]
    }
   ],
   "source": [
    "####Used for generating sythetic data\n",
    "####For the CER Dataset,Question 300 has 3 different labels,we set the input label of generator to control the output class\n",
    "#### Here we generate the data with label 3\n",
    "batch_out_data = 30000\n",
    "out_noise_3 = torch.randn((batch_out_data,100,1,1))\n",
    "out_3 = torch.ones(batch_out_data)*3\n",
    "out_3 = torch.FloatTensor(out_3).long()\n",
    "out_noise_3=out_noise_3.to(device)\n",
    "out_3 = out_3.to(device)\n",
    "fake3 = gen(out_noise_3,out_3).to(device)\n",
    "print(fake3.shape)\n",
    "L_3 = fake3.view(batch_out_data,7*48)*12.469\n",
    "L_3=L_3.cpu().detach().numpy().tolist()\n",
    "np.savetxt('./Generated_Data_30000/Q300_3.csv',L_3,delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
